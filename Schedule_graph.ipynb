{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from numpy import load, save\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import PorterStemmer, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "project = pd.read_csv ('C:/Users/yh448/OneDrive - University of Cambridge/Name_text/paper/full_short_.csv')\n",
    "project = project.rename(columns={\"Logic_y\": \"Succ_logic\", \"Lag_y\": \"Succ_lag\", \"Logic_x\": \"Pred_logic\", \"Lag_x\": \"Pred_lag\"})\n",
    "p_id = project.drop_duplicates (subset = 'Project_ID')\n",
    "p_id['Project_ID'].astype('int32')\n",
    "id_list = p_id['Project_ID'].tolist()\n",
    "print (len(id_list), type(id_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s.str.split(', ')))\n",
    "\n",
    "def full_graph (pred, logic, lag, df):\n",
    "    df[pred] = df[pred].astype(str)\n",
    "    df[logic] = df[logic].astype(str)\n",
    "    df[lag] = df[lag].astype(str)\n",
    "    a = chainer(df[pred])\n",
    "    b = chainer(df[logic])\n",
    "    c = chainer(df[lag])\n",
    "    lens = df[pred].str.split(', ').map(len)\n",
    "    graph = pd.DataFrame({pred: a, 'Clean_task_code': np.repeat(df['Clean_task_code'], lens), \n",
    "                      logic: b, lag: c,})\n",
    "    return graph\n",
    "\n",
    "def weight (logic, df, name):\n",
    "    ls_l = df[logic].tolist()\n",
    "    ls = []\n",
    "    mid = df.groupby(logic).count()[['Clean_task_code']]\n",
    "    for i in ls_l:\n",
    "        if i == 'PR_FF':\n",
    "            ls.append ((mid.loc['PR_FF', 'Clean_task_code'])/len(full_))\n",
    "        if i == 'PR_SS':\n",
    "            ls.append ((mid.loc['PR_SS', 'Clean_task_code'])/len(full_))\n",
    "        if i == 'PR_FS':\n",
    "            ls.append ((mid.loc['PR_FS', 'Clean_task_code'])/len(full_))\n",
    "        if i == 'PR_SF':\n",
    "            ls.append ((mid.loc['PR_SF', 'Clean_task_code'])/len(full_))\n",
    "        if i == 'None':\n",
    "            ls.append ((mid.loc['None', 'Clean_task_code'])/len(full_))\n",
    "        if i == 'nan':\n",
    "            ls.append ((mid.loc['nan', 'Clean_task_code'])/len(full_))\n",
    "    df[name] = ls\n",
    "    return df\n",
    "def pend_node(name1, df, name2):\n",
    "    name_ls = df[name1].tolist()\n",
    "    ls1 = []\n",
    "    for i in name_ls:\n",
    "        if i != 'nan':\n",
    "            num = df.loc[df['Clean_task_code']==i, ['Graph_x']].median()['Graph_x']\n",
    "        else: num = int(10000)\n",
    "        ls1.append (num)\n",
    "    df[name2] = ls1\n",
    "    df[name2] = df[name2].fillna(10000)\n",
    "    return df\n",
    "def group_label(name1, df, name2):\n",
    "    name_ls = df[name1].tolist()\n",
    "    ls1 = []\n",
    "    for i in name_ls:\n",
    "        g = df.loc[df['Clean_task_code']==i,]\n",
    "        if g.shape[0] != 0:\n",
    "            num = g.Foundation_detail.any()\n",
    "        else: num = 'None'\n",
    "        ls1.append (num)\n",
    "    df[name2] = ls1\n",
    "    return df\n",
    "#pred is row, succ is column, loc = row*dim+column\n",
    "def adjacency_matrix (namep, names, graph_1, weight_1, weight_2, df):\n",
    "    name1_ls = df[namep].tolist()\n",
    "    graph1 = df[graph_1].tolist()\n",
    "    weight1 = df[weight_1].tolist()\n",
    "    name2_ls = df[names].tolist()\n",
    "    weight2 = df[weight_2].tolist()\n",
    "    x = df.drop_duplicates(subset = 'Clean_task_code')\n",
    "    length = x.shape[0]\n",
    "    \n",
    "    ls1,ls2,ls3,ls4 = [],[],[],[]\n",
    "    for i, j, m in zip(name1_ls, graph1, weight1):\n",
    "        if i == 10000:\n",
    "            ls1.append (0)\n",
    "            ls2.append (0)\n",
    "        else: \n",
    "            ls1.append(i*length+j)\n",
    "            ls2.append(m)\n",
    "    for i, j, k in zip(name2_ls, graph1, weight2):\n",
    "        if i == 10000:\n",
    "            ls3.append (0)\n",
    "            ls4.append (0)\n",
    "        else: \n",
    "            ls3.append(i+j*length)\n",
    "            ls4.append(k)         \n",
    "            \n",
    "    x = np.zeros((length, length))\n",
    "    np.put(x, ls1, ls2)\n",
    "    np.put(x, ls3, ls4)\n",
    "    np.fill_diagonal(x, 1)\n",
    "    return x\n",
    "def degree_matrix(matrix):\n",
    "    ls1 = []\n",
    "    length = len(matrix)\n",
    "    x = np.zeros((length, length))\n",
    "    for i in range (length):\n",
    "        ls1.append (np.sum(matrix[i]))\n",
    "    np.fill_diagonal(x, ls1)\n",
    "    return x\n",
    "def normal_matrix (degree_m, ad_m):\n",
    "    x = np.power(degree_m.diagonal(), -0.5)\n",
    "    y = np.zeros((len(degree_m), len(degree_m)))\n",
    "    np.fill_diagonal(y, x)\n",
    "    mid = np.dot(y, ad_m)\n",
    "    a_delta = np.dot (mid, y)\n",
    "    return a_delta \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def prep (ls):\n",
    "    test_token, test_lemma, test_return = [], [], []\n",
    "    for i in range (len (ls)):\n",
    "        token = tokenize(str(ls[i]))\n",
    "        test_token.append (list(token))  \n",
    "\n",
    "    for i in range (len (test_token)):\n",
    "        x = lemmatizer.lemmatize(str(test_token[i]))\n",
    "        x1 = eval('' + x + '')\n",
    "        test_lemma.append (x1)\n",
    "    \n",
    "    for i in range (len (test_lemma)):\n",
    "        test_return.append (' '.join(test_lemma[i]))\n",
    "        \n",
    "    return test_return\n",
    "def token (ls):\n",
    "    ps = PorterStemmer()\n",
    "    token_word = []\n",
    "    for i in range (len(ls)):\n",
    "        ls1 = word_tokenize(str(ls[i]))\n",
    "        ls2 = []\n",
    "        for word in ls1:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)   \n",
    "            ls2.append (word)\n",
    "        token_word.append (ls2)\n",
    "    return token_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "test_ = pd.read_csv ('C:/Users/yh448/OneDrive - University of Cambridge/Name_text/paper/test_head.csv')\n",
    "init = np.zeros((1,1))\n",
    "for i in id_list_:\n",
    "    x2 = project.loc[project['Project_ID']== i,]\n",
    "    empty_pred = full_graph('Predecessor', 'Pred_logic', 'Pred_lag', x2)\n",
    "    empty_succ = full_graph('Successor', 'Succ_logic', 'Succ_lag',x2)\n",
    "    empty_succ = empty_succ.mask(empty_succ.eq('None')).dropna()\n",
    "    empty_pred = empty_pred.mask(empty_pred.eq('None')).dropna()\n",
    "    x2_others = x2.loc[:,['Clean_task_code','Duration', 'Project_ID','Task_name', 'Clean_task_name','WBS', 'Graph_x','r_length', 'r_pos', 'Foundation_detail']]\n",
    "    full = pd.merge (empty_pred, empty_succ, on = ('Clean_task_code'))\n",
    "    full_ = pd.merge (full, x2_others, on = 'Clean_task_code')\n",
    "    full1 = weight ('Pred_logic', full_, 'Weight_pred')\n",
    "    full2 = weight ('Succ_logic', full1, 'Weight_succ')\n",
    "    graph1 = pend_node('Predecessor', full2, 'Pred_graph')\n",
    "    graph2 = pend_node('Successor', graph1, 'Succ_graph')\n",
    "    label1 = group_label('Predecessor', graph2, 'Pred_found')\n",
    "    label2 = pend_node('Successor', label1, 'Succ_found')\n",
    "    label2['Pred_graph'] = np.where(label2.Pred_found.any() == label2.Foundation_detail.any() != 'None' , label2['Graph_x'], label2['Pred_graph'])\n",
    "    label2['Succ_graph'] = np.where(label2.Succ_found.any() == label2.Foundation_detail.any() != 'None' , label2['Graph_x'], label2['Succ_graph'])\n",
    "    test_ = test_.append (label2)\n",
    "    #amatrix = adjacency_matrix ('Pred_graph', 'Succ_graph', 'Graph_x', 'Weight_pred', 'Weight_succ', label2)\n",
    "    #print (amatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_name = test_['Clean_task_name'].tolist()\n",
    "#train_name_ = token(prep(train_name))\n",
    "#trained_model = FastText(train_name_, size = 60, min_count = 1, negative = 7, workers = 5)\n",
    "x = test_.drop_duplicates(subset = 'Clean_task_code')\n",
    "tokens = x['Clean_task_name'].tolist()\n",
    "token_ = token(prep(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_matrix (df):\n",
    "    x = df.drop_duplicates(subset = 'Clean_task_code')\n",
    "    empty = np.zeros(60).reshape(1,60)\n",
    "    ls = x['Clean_task_name'].tolist()\n",
    "    ls_token = token(prep(ls))\n",
    "    length_curr = x['r_length'].replace('None',0).tolist()\n",
    "    pos_curr  = x['r_pos'].replace('None',0).tolist()\n",
    "    x['Duration'] = x['Duration'].astype(int)\n",
    "    a = x['Duration'].to_numpy()\n",
    "    dur_curr = (np.log(a+1)).reshape(len(ls),1)\n",
    "    for i in range (len(ls)):\n",
    "        n = ' '.join(ls_token[i])\n",
    "        m = trained_model.wv[n].reshape(1,60)\n",
    "        empty = np.append (empty, m, axis = 0)\n",
    "    empty_ = np.delete(empty, 0, axis = 0)\n",
    "    len_curr = np.asarray(length_curr).reshape(len(ls),1)\n",
    "    po_curr = np.asarray(pos_curr).reshape(len(ls),1)\n",
    "    empty_ = np.append (empty_, len_curr, axis = 1)\n",
    "    empty_ = np.append (empty_, po_curr, axis = 1)\n",
    "    empty_ = np.append (empty_, dur_curr, axis = 1)\n",
    "    empty_ = empty_.astype(float)\n",
    "    return empty_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list_ = [3148, 4019, 4459, 5046, 6621, 34143, 41214, 75020, 93867, 93917, 96831, 99673, 100785, 100835, 101597, 118744, 118944, 118994, 119044, 119094, 119299, 121395, 131325, 140297, 142884]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(664, 23) (255, 255) (256, 256) (256, 63) float64 3148\n",
      "(56, 23) (24, 24) (280, 280) (280, 63) float64 4019\n",
      "(261, 23) (81, 81) (361, 361) (361, 63) float64 4459\n",
      "(538, 23) (136, 136) (497, 497) (497, 63) float64 5046\n",
      "(506, 23) (198, 198) (695, 695) (695, 63) float64 6621\n",
      "(3891, 23) (1463, 1463) (2158, 2158) (2158, 63) float64 34143\n",
      "(380, 23) (140, 140) (2298, 2298) (2298, 63) float64 41214\n",
      "(297, 23) (108, 108) (2406, 2406) (2406, 63) float64 75020\n",
      "(971, 23) (300, 300) (2706, 2706) (2706, 63) float64 93867\n",
      "(881, 23) (281, 281) (2987, 2987) (2987, 63) float64 93917\n",
      "(344, 23) (171, 171) (3158, 3158) (3158, 63) float64 96831\n",
      "(1101, 23) (457, 457) (3615, 3615) (3615, 63) float64 99673\n",
      "(1000, 23) (311, 311) (3926, 3926) (3926, 63) float64 100785\n",
      "(1051, 23) (326, 326) (4252, 4252) (4252, 63) float64 100835\n",
      "(843, 23) (291, 291) (4543, 4543) (4543, 63) float64 101597\n",
      "(1710, 23) (484, 484) (5027, 5027) (5027, 63) float64 118744\n",
      "(1901, 23) (512, 512) (5539, 5539) (5539, 63) float64 118944\n",
      "(1181, 23) (282, 282) (5821, 5821) (5821, 63) float64 118994\n",
      "(1959, 23) (519, 519) (6340, 6340) (6340, 63) float64 119044\n",
      "(1776, 23) (495, 495) (6835, 6835) (6835, 63) float64 119094\n",
      "(686, 23) (299, 299) (7134, 7134) (7134, 63) float64 119299\n",
      "(2377, 23) (925, 925) (8059, 8059) (8059, 63) float64 121395\n",
      "(1632, 23) (330, 330) (8389, 8389) (8389, 63) float64 131325\n",
      "(3998, 23) (543, 543) (8932, 8932) (8932, 63) float64 140297\n",
      "(153, 23) (85, 85) (9017, 9017) (9017, 63) float64 142884\n"
     ]
    }
   ],
   "source": [
    "init = np.zeros((1,1))\n",
    "f_init = np.zeros((1,63))\n",
    "for i in id_list_:\n",
    "    x2 = test_.loc[test_['Project_ID']==i,]\n",
    "    amatrix = adjacency_matrix ('Pred_graph', 'Succ_graph', 'Graph_x', 'Weight_pred', 'Weight_succ', x2)\n",
    "    A = init.shape[0]\n",
    "    B = amatrix.shape[0]\n",
    "    init = np.block([[init,np.zeros((A,B))],[np.zeros((B,A)), amatrix]])\n",
    "    feature = feature_matrix(x2)\n",
    "    f_init = np.append (f_init, feature, axis=0)\n",
    "    print(x2.shape, amatrix.shape, init.shape, f_init.shape, feature.dtype, i)\n",
    "init_ = np.delete(init, 0, 0)\n",
    "init_f = np.delete(init_, 0, 1)\n",
    "degree_m = degree_matrix (init_f)\n",
    "normal_m = normal_matrix (degree_m, init_f)\n",
    "feature_m = np.delete(f_init, 0, 0)\n",
    "train_in = np.dot (normal_m, feature_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30157, 23) (9016, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_nodup = test_.drop_duplicates(subset = ('Clean_task_code', 'Project_ID'))\n",
    "test_nodup['Foundation_detail'] = test_nodup['Foundation_detail'].astype('category')\n",
    "test_nodup[\"Ont_label\"] = test_nodup['Foundation_detail'].cat.codes\n",
    "print (test_.shape, test_nodup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yh448\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "batch_size = [20, 40, 60, 70, 80]\n",
    "epochs = [60, 70, 80, 90, 100]\n",
    "optimizer = ['SGD']\n",
    "neurons = [50,70,100,120,150]\n",
    "\n",
    "def create_model(optimizer='sgd', neurons = 20): \n",
    "    gcn_model = models.Sequential()\n",
    "    gcn_model.add(layers.Dense(neurons, activation='relu', input_dim = 63))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='softmax'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='relu'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='softmax'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='relu'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='softmax'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='relu'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='softmax'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='relu'))\n",
    "    gcn_model.add(layers.Dense(neurons, activation='softmax'))\n",
    "    gcn_model.add(layers.Dense(1))\n",
    "    #optimizer = tf.keras.optimizers.SGD(lr=learn_rate, momentum=momentum)\n",
    "    gcn_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return gcn_model\n",
    "\n",
    "gcn_model = KerasClassifier(build_fn = create_model, verbose = 0)\n",
    "param_grid_1 = dict(batch_size = batch_size, epochs = epochs, optimizer=optimizer, \n",
    "                    neurons = neurons)#\n",
    "grid_1 = GridSearchCV(estimator = gcn_model, param_grid=param_grid_1, \n",
    "                      n_jobs=-1, cv=5, scoring='f1_macro')\n",
    "label_x = test_nodup['Ont_label'].to_numpy()\n",
    "grid_result = grid_1.fit(train_in, label_x)\n",
    "best = grid_result.best_estimator_\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print (classification_report(label_x, best.predict(train_in)))\n",
    "print (confusion_matrix(label_x, best.predict(train_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      1.00      0.01         9\n",
      "           1       0.00      0.00      0.00        35\n",
      "           2       0.00      0.00      0.00      2238\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.00      2297\n",
      "   macro avg       0.00      0.20      0.00      2297\n",
      "weighted avg       0.00      0.00      0.00      2297\n",
      "\n",
      "[[   9    0    0    0    0]\n",
      " [  35    0    0    0    0]\n",
      " [2238    0    0    0    0]\n",
      " [  10    0    0    0    0]\n",
      " [   5    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(label_x, best.predict(train_in)))\n",
    "print (confusion_matrix(label_x, best.predict(train_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ont_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foundation_detail</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Base slab</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Floor slab</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foundation</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lift pit</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mat</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>8869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pad</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parapet Wall</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pile</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roof</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strip</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wall</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ont_label\n",
       "Foundation_detail           \n",
       "Base slab                  9\n",
       "Floor slab                 6\n",
       "Foundation                11\n",
       "General                    2\n",
       "Lift pit                  37\n",
       "Mat                       28\n",
       "None                    8869\n",
       "Pad                        2\n",
       "Parapet Wall               8\n",
       "Pile                      25\n",
       "Roof                       1\n",
       "Strip                      6\n",
       "Wall                      12"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nodup.groupby('Foundation_detail').count()[['Ont_label']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
